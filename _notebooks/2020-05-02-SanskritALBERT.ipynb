{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-05-02-SanskritALBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jL1ankzsNLlnQbACZVkGyOYHDgZxeSEg",
      "authorship_tag": "ABX9TyO9I4DQkn/hf8wf0u4F3zYu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntd1jf7bzC9J",
        "colab_type": "text"
      },
      "source": [
        "# \"Sanskrit Albert\"\n",
        "> \"Training a Language model from scratch on Sanskrit using the HuggingFace library, and how to train your own model too!\"\n",
        "\n",
        "- toc: true \n",
        "- comments: true\n",
        "- categories: [jupyter, NLP, HuggingFace]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKVtBr98lMmL",
        "colab_type": "code",
        "outputId": "63ec7676-b2d8-4e61-f7a3-24899d57d200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  2 06:43:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXFhPINv-rKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_ImQQX3BVfN",
        "colab_type": "text"
      },
      "source": [
        "HuggingFace Recently updated their scripts, and the pip is yet to be released. So We'll build from source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWMe_m6iIw5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tokenizers\n",
        "#!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-eppuIcNyWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LHPg0HvFcmB",
        "colab_type": "text"
      },
      "source": [
        "# Collecting Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B11LM6T5T8_M",
        "colab_type": "text"
      },
      "source": [
        "I have used Sanskrit Corpus from Kaggle dataset. Feel free to skip and use your own ddataset. The trainig data needs to be in a .txt file. and I have also used Evaluation using the same dataset. \n",
        "\n",
        "I need Kaggle API to download the dataset. You can load your text corpus from anywhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l98kBADHZGb",
        "colab_type": "text"
      },
      "source": [
        "You can download corpus for your language from https://traces1.inria.fr/oscar.\n",
        "\n",
        "I have used data fro mthere too, and appended the data to corpus from Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Z6oW1oCBrl",
        "colab_type": "text"
      },
      "source": [
        "## Loading From kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvchHy610dzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/ \n",
        "!chmod 600 ~/.kaggle/kaggle.json "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ZY6uLn2kTP",
        "colab_type": "text"
      },
      "source": [
        "[Kaggle dataset link](https://www.kaggle.com/disisbig/sanskrit-wikipedia-articles)\n",
        "\n",
        "Thanks to [inltk](https://github.com/goru001/inltk) fro wikipedia dumps\n",
        "and [CLTK](https://github.com/cltk/cltk) from which I am currently collecting Sanskrit scraps from open sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwRdO6o20Wt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir corpus \n",
        "#directory for sac=ving all corpus in a single directory. You can save it anywhere\n",
        "\n",
        "#From Kagle\n",
        "!kaggle datasets download -d disisbig/sanskrit-wikipedia-articles\n",
        "!unzip /content/sanskrit-wikipedia-articles.zip -d /content/corpus\n",
        "\n",
        "#From OSCAR corpus\n",
        "!wget https://traces1.inria.fr/oscar/files/compressed-orig/sa.txt.gz\n",
        "!gunzip /content/sa.txt.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Ndf0bMJm0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading sample\n",
        "with open(\"/content/sa.txt\", \"r\") as fp:\n",
        "    print(fp.read(1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SluFUKP1vVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "train_list = glob.glob(\"/content/corpus/train/train/*.txt\")\n",
        "valid_list = glob.glob(\"/content/corpus/valid/valid/*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjO-jxoc4NU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#readig and appending all small files to single Train and Valid files\n",
        "with open(\"/content/corpus/train/full.txt\", \"wb\") as outfile:\n",
        "    for f in train_list:\n",
        "        with open(f, \"rb\") as infile:\n",
        "            outfile.write(infile.read())\n",
        "            outfile.write(b\"\\n\\n\")\n",
        "    with open(\"/content/sa.txt\", \"rb\") as infile:\n",
        "            outfile.write(infile.read())\n",
        "\n",
        "with open(\"/content/corpus/valid/full_val.txt\", \"wb\") as outfile:\n",
        "    for f in valid_list:\n",
        "        with open(f, \"rb\") as infile:\n",
        "            outfile.write(infile.read())\n",
        "            outfile.write(b\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8kXpzNECGBx",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HURGd282Ib9Z",
        "colab_type": "text"
      },
      "source": [
        "Directory to save trained tokenier and configuration files in a folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S9cL1otlalH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpLtcQ2K1C3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sentencepiece as spm\n",
        "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ZO2jfCRXm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "#Albert Tokenizer uses Sentence piece Tokenization, so I have used sentencepiece to to train tokenizer.\n",
        "#This will take a while\n",
        "spm.SentencePieceTrainer.Train('--input=/content/corpus/train/full.txt \\\n",
        "                                --model_prefix=m \\\n",
        "                                --vocab_size=32000 \\\n",
        "                                --control_symbols=[CLS],[SEP],[MASK]')\n",
        "\n",
        "with open(\"m.vocab\") as v:\n",
        "    print(v.read(2000))\n",
        "    v.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jMAPtFRbsS",
        "colab_type": "code",
        "outputId": "aa109b9e-1ade-4a25-9ad8-07581f1284cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir /content/data_dir/\n",
        "!cp /content/m.model -d /content/data_dir/spiece.model\n",
        "!cp /content/m.vocab -d /content/data_dir/spiece.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜/content/data_dir/â€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4_m4l83CoU7",
        "colab_type": "text"
      },
      "source": [
        "## Testing Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqikic-zJTjS",
        "colab_type": "text"
      },
      "source": [
        "Make sure to checkout the Fast Tokenizers from Huggingface, Tis is really Fast!\n",
        "You can compare with sentencepiece."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wok5q40x1Cyl",
        "colab_type": "code",
        "outputId": "4d8df1d8-8a3c-4327-89b2-a4c3a757965b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%time\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer.train(\"/content/corpus/train/full.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 Âµs, sys: 0 ns, total: 4 Âµs\n",
            "Wall time: 10 Âµs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHRcs4XmGs2o",
        "colab_type": "text"
      },
      "source": [
        "This is a very beautiful Shlok â¤ï¸, Let's just pray for this ðŸ™.\n",
        "Do search the quotes used in this notebook, I am sure, you will love them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2G6ptseOWWS",
        "colab_type": "code",
        "outputId": "bed72c72-3793-4606-9130-4db512ebdc00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "txt = \"à¥ à¤¸à¤°à¥à¤µà¥‡à¤¤à¥à¤° à¤¸à¥à¤–à¤¿à¤¨à¤ƒ à¤¸à¤¨à¥à¤¤à¥| à¤¸à¤°à¥à¤µà¥‡ à¤¸à¤¨à¥à¤¤à¥ à¤¨à¤¿à¤°à¤¾à¤®à¤¯à¤¾à¤ƒ| à¤¸à¤°à¥à¤µà¥‡ à¤­à¤¦à¥à¤°à¤¾à¤£à¤¿ à¤ªà¤¶à¥à¤¯à¤¨à¥à¤¤à¥| à¤®à¤¾à¤ à¤•à¤¶à¥à¤šà¤¿à¤¦à¥ à¤¦à¥à¤ƒà¤– à¤®à¤¾à¤ªà¥à¤¨à¥à¤¯à¤¾à¤¤à¥¥ à¥ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¥¥\"\n",
        "enc = tokenizer.encode(txt)\n",
        "print(tokenizer.decode(enc.ids))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "à¥ à¤¸à¤°à¥à¤µà¥‡à¤¤à¥à¤° à¤¸à¥à¤–à¤¿à¤¨à¤ƒ à¤¸à¤¨à¥à¤¤à¥| à¤¸à¤°à¥à¤µà¥‡ à¤¸à¤¨à¥à¤¤à¥ à¤¨à¤¿à¤°à¤¾à¤®à¤¯à¤¾à¤ƒ| à¤¸à¤°à¥à¤µà¥‡ à¤­à¤¦à¥à¤°à¤¾à¤£à¤¿ à¤ªà¤¶à¥à¤¯à¤¨à¥à¤¤à¥| à¤®à¤¾à¤ à¤•à¤¶à¥à¤šà¤¿à¤¦à¥ à¤¦à¥à¤ƒà¤– à¤®à¤¾à¤ªà¥à¤¨à¥à¤¯à¤¾à¤¤à¥¥ à¥ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¤¶à¤¾à¤‚à¤¤à¤¿à¤ƒ à¥¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2NsYgCNP4Xy",
        "colab_type": "text"
      },
      "source": [
        "The tokenizer ssems to work, But since, The training script is configured to use Albert tokenizer. we need to use spiece.model and spiece.vocab, for training script\n",
        "\n",
        "HuggingFace tokenizer creates `['/content/hft/vocab.json', '/content/hft/merges.txt']`\n",
        "\n",
        "files, while the AlbertTokenizer requires spiece.model file. So we'll use sentencepiece saved vocab and tokenizer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlaRWSl26Hw",
        "colab_type": "code",
        "outputId": "1b038dee-fc57-469b-efd8-39417d006506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir hft\n",
        "tokenizer.save(\"/content/hft\")\n",
        "#we won't be using this"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/hft/vocab.json', '/content/hft/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUu9C3cBD3Rm",
        "colab_type": "text"
      },
      "source": [
        "# Huggingface Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-LXoQM_FUS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNAOMXjpMHZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Keep in mind, This is a tokenizer for Albert, unlike the previous one, which is a generic one.\n",
        "#We'll load it in the form of Albert Tokenizer.\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"/content/data_dir\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fshtRIT71G49",
        "colab_type": "code",
        "outputId": "e6653e21-b172-478d-cff6-13f325b561ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "op = tokenizer.encode(\"à¤¨à¥ˆà¤¨à¤‚ à¤›à¤¿à¤¨à¥à¤¦à¤¨à¥à¤¤à¤¿ à¤¶à¤¸à¥à¤¤à¥à¤°à¤¾à¤£à¤¿ à¤¨à¥ˆà¤¨à¤‚ à¤¦à¤¹à¤¤à¤¿ à¤ªà¤¾à¤µà¤•à¤ƒà¥¤ à¤¨ à¤šà¥ˆà¤¨à¤‚ à¤•à¥à¤²à¥‡à¤¦à¤¯à¤¨à¥à¤¤à¥à¤¯à¤¾à¤ªà¥‹ à¤¨ à¤¶à¥‹à¤·à¤¯à¤¤à¤¿ à¤®à¤¾à¤°à¥à¤¤à¤ƒà¥¥\")\n",
        "tokenizer.decode(op)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] à¤¨à¥ˆà¤¨à¤‚ à¤›à¤¿à¤¨à¥à¤¦à¤¨à¥à¤¤à¤¿ à¤¶à¤¸à¥à¤¤à¥à¤°à¤¾à¤£à¤¿ à¤¨à¥ˆà¤¨à¤‚ à¤¦à¤¹à¤¤à¤¿ à¤ªà¤¾à¤µà¤•à¤ƒà¥¤ à¤¨ à¤šà¥ˆà¤¨à¤‚ à¤•à¥à¤²à¥‡à¤¦à¤¯à¤¨à¥à¤¤à¥à¤¯à¤¾à¤ªà¥‹ à¤¨ à¤¶à¥‹à¤·à¤¯à¤¤à¤¿ à¤®à¤¾à¤°à¥à¤¤à¤ƒà¥¥[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vknw7FwU5oQ1",
        "colab_type": "text"
      },
      "source": [
        "Looks like, the tokenizer is working"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXX8IPifHDuk",
        "colab_type": "text"
      },
      "source": [
        "## Model-Tokenizer Configurtion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cznfudYuHI9F",
        "colab_type": "text"
      },
      "source": [
        "This is important.\n",
        "The training script needs a configuration for the model.\n",
        "\n",
        "Architecture refers to what the model is going to be used for\\\n",
        "i.e., AlbertModelForLM, or for Sequence Classification.\n",
        "Just take a look ar left panel for Model Architectures\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXbOamDnLL_b",
        "colab_type": "code",
        "outputId": "7c37a1ed-817d-470c-c3d6-33e8a97795bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEBZjCRKItM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(\"/content/data_dir/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note I havve set do_lower_case: False, and keep_accents:True\n",
        "\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(\"/content/data_dir/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KUdxTNaD9k6",
        "colab_type": "text"
      },
      "source": [
        "**Note: **While experimenting with tokenizer training, I found that encoding was done corectly, but when decoding with {do_lower_case: True, and keep_accents:False}, the decoded sentence was a bit changed.\n",
        "\n",
        "So, by using above settings, I got the sentences decoded perfectly. \n",
        "a reason maybe that Sanskrit does not have 'Casing'. and the word has suffixes in the form of accents.\n",
        "\n",
        "You should try with the settings ehich suits best for your langugae."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPCraE3SzTY",
        "colab_type": "code",
        "outputId": "b0ee231e-bf6b-460c-935e-4871868675d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC8sFtOcE2-0",
        "colab_type": "text"
      },
      "source": [
        "Creating a small corpus for testing, You can skip this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIIQtIXCTUMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/corpus/train/tmp.txt\", \"w\") as fp:\n",
        "    fp.write(open(\"/content/corpus/train/full.txt\", \"r\").read(100000))      #250KB\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe-5GKB6hivO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/corpus/valid/val_val.txt\", \"w\") as fp:\n",
        "    fp.write(open(\"/content/corpus/valid/full_val.txt\", \"r\").read(10000000)) #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7v8tpoNE9-T",
        "colab_type": "text"
      },
      "source": [
        "Checkpointing is very important. This is a directory where the intermediate model and tokenizer will be saved. \n",
        "\n",
        "**Note:** You should checkpoint to somewhere else, Maybe to your drive. and set \n",
        "`--save_total_limit 2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aHxYqnQFOLK",
        "colab_type": "text"
      },
      "source": [
        "This is the training script. you should experiment with arguments.\n",
        " \n",
        "`!python /content/transformers/examples/run_language_modeling.py --help`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIrIMRdTHwj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqSpJ-kUBjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFoA9BSfVkIs",
        "colab_type": "text"
      },
      "source": [
        "You see the magic here.\n",
        "\n",
        "This script can be used to train most models with for Language modelling.\n",
        "\n",
        "Another thing, Observe that you have to directly specify `--training_data_file` in `.txt` format. No need to generate any pretraining data! all thanks to the Fast toknizers in used for loading the text.\n",
        "\n",
        "Features are created dynamically while starting trainng script.\n",
        "However, This is limited to GPUs only. I would love to see a TPU version too.\n",
        "\n",
        "Make sure to change batch_sizes according to the GPU you are having. I set to 16 because of 8 GB P4, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU_mXJAtD20N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To train from scratch\n",
        "!python /content/transformers/examples/run_language_modeling.py \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/data_dir/ \\\n",
        "        --tokenizer_name /content/data_dir/ \\\n",
        "        --train_data_file /content/corpus/train/full.txt \\\n",
        "        --eval_data_file /content/corpus/valid/full_val.txt \\\n",
        "        --output_dir /content/data_dir \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --mlm \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --evaluate_during_training \\\n",
        "        --num_train_epochs 5 \\\n",
        "        --per_gpu_eval_batch_size 16 \\\n",
        "        --per_gpu_train_batch_size 16 \\\n",
        "        --block_size 256 \\\n",
        "        --seed 108 \\\n",
        "        --should_continue \\\n",
        "        --logging_dir logs \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz-I_cW6kL4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvVR9O8NFlvB",
        "colab_type": "text"
      },
      "source": [
        "**Continuing Training**\n",
        "\n",
        "```\n",
        "--model_name_or_path      #Refers to the checkpoint directory\n",
        "--overwrite_output_dir    #This is used to continue fro mlast checkpoint\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwlU22Ca1Vs",
        "colab_type": "text"
      },
      "source": [
        "After a checkpoint, You just need that directory and the corpus files, and toknizer. All configs, models, oprimizers are saved in `--output_dir` except tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jwWe5aPM452",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To continue from checkpoint\n",
        "#I have continued from 500 steps here, but you should use the latet saved models\n",
        "!python /content/transformers/examples/run_language_modeling.py \\\n",
        "        --model_name_or_path /content/data_dir/checkpoint-500 \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/data_dir/ \\\n",
        "        --tokenizer_name /content/data_dir/ \\\n",
        "        --train_data_file /content/corpus/train/full.txt \\\n",
        "        --eval_data_file /content/corpus/valid/full_val.txt \\\n",
        "        --output_dir /content/data_dir \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --mlm \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 5 \\\n",
        "        --evaluate_during_training \\\n",
        "        --per_gpu_eval_batch_size 64 \\\n",
        "        --per_gpu_train_batch_size 64 \\\n",
        "        --block_size 256 \\\n",
        "        --seed 108 \\\n",
        "        --should_continue \\\n",
        "        --overwrite_output_dir \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIQBNZchqNIv",
        "colab_type": "text"
      },
      "source": [
        "# Saving for Uploading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5koYK6qTfy",
        "colab_type": "text"
      },
      "source": [
        "Since, training is complete, We can now upload models to Huffingface's \n",
        "[Models](https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6NpJ_W6qJMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir sanskrit_albert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNj7McmDNdRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "01d67521-2940-4131-bb4e-7c2edf0d79d1"
      },
      "source": [
        "atokenizer = AlbertTokenizer.from_pretrained(\"/content/data_dir\")\n",
        "atokenizer.save_pretrained(\"/content/sanskrit_albert\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/sanskrit_albert/spiece.model',\n",
              " '/content/sanskrit_albert/special_tokens_map.json',\n",
              " '/content/sanskrit_albert/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRo1M23hEXoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7628b76-b55b-4f52-a624-f8ca2a7db5bc"
      },
      "source": [
        "op = atokenizer.encode(\"à¤•à¥à¤·à¤®à¤¯à¤¾ à¤¦à¤¯à¤¯à¤¾ à¤ªà¥à¤°à¥‡à¤®à¥à¤£à¤¾ à¤¸à¥‚à¤¨à¥ƒà¤¤à¥‡à¤¨à¤¾à¤°à¥à¤œà¤µà¥‡à¤¨ à¤šà¥¤ à¤µà¤¶à¥€à¤•à¥à¤°à¥à¤¯à¤¾à¤œà¥à¤œà¤—à¤¤à¥à¤¸à¤°à¥à¤µà¤‚ à¤µà¤¿à¤¨à¤¯à¥‡à¤¨ à¤š à¤¸à¥‡à¤µà¤¯à¤¾à¥¥\")\n",
        "print(atokenizer.decode(op))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] à¤•à¥à¤·à¤®à¤¯à¤¾ à¤¦à¤¯à¤¯à¤¾ à¤ªà¥à¤°à¥‡à¤®à¥à¤£à¤¾ à¤¸à¥‚à¤¨à¥ƒà¤¤à¥‡à¤¨à¤¾à¤°à¥à¤œà¤µà¥‡à¤¨ à¤šà¥¤ à¤µà¤¶à¥€à¤•à¥à¤°à¥à¤¯à¤¾à¤œà¥à¤œà¤—à¤¤à¥à¤¸à¤°à¥à¤µà¤‚ à¤µà¤¿à¤¨à¤¯à¥‡à¤¨ à¤š à¤¸à¥‡à¤µà¤¯à¤¾à¥¥[SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPp5IPrBiQgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I am using chackoint because os not much training\n",
        "model = AlbertModel.from_pretrained(\"/content/data_dir/checkpoint-500\")\n",
        "model.save_pretrained(\"/content/sanskrit_albert\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvrOXAJIrD0P",
        "colab_type": "text"
      },
      "source": [
        "Now All the files we want are in a separate folder, Which is all we need to upoad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74xv9bX_sHQb",
        "colab_type": "text"
      },
      "source": [
        "### **Tests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmInFtWZOKqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(\"/content/sanskrit_albert\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGBejEJTrjSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt = \"à¤šà¤°à¤¨à¥à¤®à¤¾à¤°à¥à¤—à¤¾à¤¨à¥à¤µà¤¿à¤œà¤¾à¤¨à¤¾à¤¤à¤¿ à¥¤\"\n",
        "op = tokenizer.encode(txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYkNSiRo7zGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d6553c1-9be5-4d4a-8dae-3dfee2e28604"
      },
      "source": [
        "op\n",
        "#See howw it's tokenized!"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 15, 4280, 1345, 82, 177, 13866, 6, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytuIElHlNbat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a0ef3e0-f4c7-4738-a9a5-783fb54788cf"
      },
      "source": [
        "tokenizer.decode(op[:5]), tokenizer.decode(op[5:])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] à¤šà¤°à¤¨à¥à¤®à¤¾à¤°à¥à¤—à¤¾à¤¨à¥', 'à¤µà¤¿à¤œà¤¾à¤¨à¤¾à¤¤à¤¿ à¥¤[SEP]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u8UhDds8DYZ",
        "colab_type": "text"
      },
      "source": [
        "This is the reason I set `do_lower_case:False`, and `keep_accents:True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbdNOO2uMxo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = model(torch.tensor(op).unsqueeze(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coI3WfZhiQcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af9e7970-3f1f-46be-c891-f0c1496d4121"
      },
      "source": [
        "print(ps[0].shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([30, 1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ADrAIUr5FR",
        "colab_type": "text"
      },
      "source": [
        "This way you can get the embeddings for a sentence. Check [ReSanskrit](https://resanskrit.com/sanskrit-shlok-popular-quotes-meaning-hindi-english/) for some beautiful shlok quotes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62oFJU2r9tn",
        "colab_type": "text"
      },
      "source": [
        "## Uploading to Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwibhkZOvH-s",
        "colab_type": "text"
      },
      "source": [
        "[Instructions to upload a model](https://github.com/huggingface/transformers#Quick-tour-of-model-sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35VdrMskA1aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!transformers-cli login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHpZEiTJvOzx",
        "colab_type": "text"
      },
      "source": [
        "Make sure your model name is the folder name in which this will be uploaded.\n",
        "\n",
        "Thus, my model would be `surajp/sanskrit_albert`,\n",
        "but I won't upload this as I have alreasy uploaded one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkn-gHRZA1U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!transformers-cli upload /content/sanskrit_albert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4sJhjqGu94C",
        "colab_type": "text"
      },
      "source": [
        "And It's done! Since, I have alreadu uploaded a model, You can load using `surajp/sanskrit-base-albert`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5bVD9EVu9oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this way\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"surajp/albert-base-sanskrit\")\n",
        "model = AutoModel.from_pretrained(\"surajp/albert-base-sanskrit\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqihFZk9u9lY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f437ae1d-d201-490d-f85a-c8a08d3bfa31"
      },
      "source": [
        "enc=tokenizer.encode(\"à¤…à¤ªà¤¿ à¤¸à¥à¤µà¤°à¥à¤£à¤®à¤¯à¥€ à¤²à¤™à¥à¤•à¤¾ à¤¨ à¤®à¥‡ à¤²à¤•à¥à¤·à¥à¤®à¤£ à¤°à¥‹à¤šà¤¤à¥‡ à¥¤ à¤œà¤¨à¤¨à¥€ à¤œà¤¨à¥à¤®à¤­à¥‚à¤®à¤¿à¤¶à¥à¤š à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤¦à¤ªà¤¿ à¤—à¤°à¥€à¤¯à¤¸à¥€ à¥¥\")\n",
        "print(tokenizer.decode(enc))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] à¤…à¤ªà¤¿ à¤¸à¥à¤µà¤°à¥à¤£à¤®à¤¯à¥€ à¤²à¤™à¥à¤•à¤¾ à¤¨ à¤®à¥‡ à¤²à¤•à¥à¤·à¥à¤®à¤£ à¤°à¥‹à¤šà¤¤à¥‡ à¥¤ à¤œà¤¨à¤¨à¥€ à¤œà¤¨à¥à¤®à¤­à¥‚à¤®à¤¿à¤¶à¥à¤š à¤¸à¥à¤µà¤°à¥à¤—à¤¾à¤¦à¤ªà¤¿ à¤—à¤°à¥€à¤¯à¤¸à¥€ à¥¥[SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3PZQGn6qNCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = model(torch.tensor(enc).unsqueeze(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUpccuvdwh94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34e0b650-6144-44d7-894c-cae9c5c480aa"
      },
      "source": [
        " ps[0].shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([19, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grbPm04A_eZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_pretrained(\"./\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEEh_B3aw73P",
        "colab_type": "text"
      },
      "source": [
        "I hope This notebook was helpful.ðŸ¤—\n",
        "    #StaySafe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9vsOsCxxbAa",
        "colab_type": "text"
      },
      "source": [
        "This training contained only a little portion of Sanskrit literature. There is a vast amount of literature there which I am collecting. This was only a checkpoint for trainng, I will train more once I collect more data.\n",
        "\n",
        "I am also trainig for other Indian Languages on different models (Gujarati, Hindi for now).\n",
        "\n",
        "If you know any resources, Please write to me. I'd love to have your contribution.\n",
        "\n",
        "**`parmarsuraj99@gmail.com`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596xiihF59SA",
        "colab_type": "text"
      },
      "source": [
        "I am trying to find if the structure of language can have any effect on trainng, More structured language=>faster training and if this can be useful for cross-lingual learning?\n",
        "\n",
        "What are you thoughts about this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PezqKl0BwxmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}