{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parmarsuraj99/10DaysofMLChallenge/blob/master/notebopoks/COMP8730_proposed_solution_author_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFVjZS8JAkY7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.18.0\n",
        "!pip install datasets==2.0.0\n",
        "!pip install apache_beam==2.37.0\n",
        "!pip install sentencepiece==0.1.96\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXQshprDfpk8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/parmarsuraj99/COMP8730_research_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y64NzBRMpsPA"
      },
      "outputs": [],
      "source": [
        "%cd /content/COMP8730_research_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPf9tTj96LhE"
      },
      "source": [
        "## Author prediction training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "U3ITccd73Bah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download_folder(\"https://drive.google.com/drive/folders/1vdloyc7skwlIAN5bEG7JdI4Pyu6JaXBU\")"
      ],
      "metadata": {
        "id": "oh2xqAoDfhgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zjSEnVbBfHHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def prepare_csv(file_dir=\"inltk_sanskrit_shlokas_dataset\"):\n",
        "\n",
        "    df = pd.read_csv(f\"{file_dir}/train.csv\")\n",
        "    df_test = pd.read_csv(f\"{file_dir}/valid.csv\")\n",
        "\n",
        "    train_ = df.copy()\n",
        "    test_ = df_test.copy()\n",
        "\n",
        "    enc = LabelEncoder()\n",
        "    train_[\"Class\"] = enc.fit_transform(df[\"Class\"])\n",
        "    test_[\"Class\"] = enc.transform(df_test[\"Class\"])\n",
        "\n",
        "    train_.rename(columns={\"Class\": \"label\", \"Sloka\":\"text\"}, inplace=True)\n",
        "    test_.rename(columns={\"Class\": \"label\", \"Sloka\":\"text\"}, inplace=True)\n",
        "\n",
        "    train_[\"text\"] = train_[\"text\"].str.replace(' +', ' ')\n",
        "    test_[\"text\"] = test_[\"text\"].str.replace(' +', ' ')\n",
        "\n",
        "    train_.to_csv(\"train_processed.csv\", index=False)\n",
        "    test_.to_csv(\"test_processed.csv\", index=False)\n",
        "\n",
        "prepare_csv()"
      ],
      "metadata": {
        "id": "hStTTYTpP1jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB57JKj3BoKx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import glob, os, gc\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_metric\n",
        "import json\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "freeze_all=False\n",
        "\n",
        "dataset = load_dataset('csv', data_files={\"train\": 'train_processed.csv', \"test\":\"test_processed.csv\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_all = False"
      ],
      "metadata": {
        "id": "mxki-wms54EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5SlDr67FNAB"
      },
      "outputs": [],
      "source": [
        "for postfix_ in [\"True\", \"False\"]:\n",
        "    sorted_files = sorted(\n",
        "        glob.glob(\n",
        "            f\"/content/COMP8730_research_project/COMP8730_NLPU/*/results_scratch_{postfix_}/checkpoint*\"\n",
        "        ),\n",
        "        key=lambda x: int(x.split(\"-\")[1]),\n",
        "    )\n",
        "\n",
        "    for i in range(len(sorted_files)):\n",
        "\n",
        "        sorted_files_index = i\n",
        "        config_name = (\n",
        "            sorted_files[sorted_files_index].split(\"/\")[-2]\n",
        "            + \"_\"\n",
        "            + sorted_files[sorted_files_index].split(\"/\")[-1]\n",
        "        ).replace(\"-\", \"_\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(sorted_files[sorted_files_index])\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            sorted_files[sorted_files_index], num_labels=3\n",
        "        )\n",
        "        gc.collect()\n",
        "\n",
        "        clear_output()\n",
        "        gc.collect()\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(\n",
        "                examples[\"text\"].replace(\"\\n\", \"\").replace(\"\\t\", \"\"),\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "            )\n",
        "\n",
        "        tokenized_datasets = dataset.map(tokenize_function)\n",
        "\n",
        "        if not freeze_all:\n",
        "            for name, param in list(model.albert.named_parameters())[:-5]:\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            model.albert.requires_grad_(False)\n",
        "\n",
        "        metric = load_metric(\"f1\")\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            metric1 = load_metric(\"precision\")\n",
        "            metric2 = load_metric(\"recall\")\n",
        "            metric_f1 = load_metric(\"f1\")\n",
        "            metric_acc = load_metric(\"accuracy\")\n",
        "            \n",
        "            logits, labels = eval_pred\n",
        "            predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "            precision = metric1.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n",
        "            recall = metric2.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n",
        "            f1 = metric_f1.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n",
        "            accuracy = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "            return {\"precision\": precision, \"recall\": recall, \"f1\":f1, \"accuracy\":accuracy}\n",
        "\n",
        "        op_dir = (\n",
        "            sorted_files[sorted_files_index].split(\"/\")[-2]\n",
        "            + \"_\"\n",
        "            + sorted_files[sorted_files_index].split(\"/\")[-1]\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=op_dir,\n",
        "            num_train_epochs=10,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            logging_strategy=\"epoch\",\n",
        "            optim=\"adamw_torch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=1,\n",
        "            learning_rate=1e-6,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            data_seed=0,\n",
        "            load_best_model_at_end=True,\n",
        "        )\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        gc.collect()\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_datasets[\"train\"],\n",
        "            eval_dataset=tokenized_datasets[\"test\"],\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        gc.collect()\n",
        "\n",
        "        results_dict = dict()\n",
        "\n",
        "        train_stats = trainer.train()\n",
        "        results_dict[\"train_stats\"] = train_stats\n",
        "        gc.collect()\n",
        "\n",
        "        eval_scores = trainer.evaluate()\n",
        "        results_dict[\"eval_scores\"] = eval_scores\n",
        "        gc.collect()\n",
        "\n",
        "        train_scores = trainer.evaluate(tokenized_datasets[\"train\"])\n",
        "        results_dict[\"train_scores\"] = train_scores\n",
        "        gc.collect()\n",
        "\n",
        "        eval_preds = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "        eval_tgt = np.array(tokenized_datasets[\"test\"][\"label\"])\n",
        "\n",
        "        with open(f\"{config_name}.json\", \"w\") as fp:\n",
        "            json.dump(results_dict, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_xnLqRhw5FDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "J7mW0BtRrzQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waP3qY2NIPFr"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"results.zip\", 'w') as myZip:\n",
        "    for name in glob.glob(\"*.json\" ):\n",
        "        myZip.write(name, os.path.basename(name), zipfile.ZIP_DEFLATED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = {}\n",
        "for file_ in glob.glob(\"*.json\"):\n",
        "    with open(file_, \"r\") as fp:\n",
        "        res = json.load(fp)\n",
        "    tmp = {}\n",
        "    tmp[\"eval_f1\"] = res[\"eval_scores\"][\"eval_f1\"]\n",
        "    results_dict[file_.split(\".\")[0]] = tmp"
      ],
      "metadata": {
        "id": "JaeFYLIXvvMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict"
      ],
      "metadata": {
        "id": "pwUefXNRtQXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_df = pd.DataFrame.from_dict(results_dict).T"
      ],
      "metadata": {
        "id": "aQgrsq2yrQF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_z0NSw7FyzJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res_df.loc[[c for c in res_df.index.to_list() if \"False\" in c]].mean())\n",
        "res_df.loc[[c for c in res_df.index.to_list() if \"False\" in c]].plot.barh(figsize=(10,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "apqqonbXua2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res_df.loc[[c for c in res_df.index.to_list() if \"True\" in c]].mean())\n",
        "res_df.loc[[c for c in res_df.index.to_list() if \"True\" in c]].plot.barh(figsize=(10,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qCNVGML1vvIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QDpbahhqt5oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading to HF Hub\n",
        "\n",
        "optional if you'd like to share your models"
      ],
      "metadata": {
        "id": "RsJeLpQk2fVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!sudo apt-get install git-lfs"
      ],
      "metadata": {
        "id": "pJmciFNe2hb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8vZmZuU2m2x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ao7hMMW2m2y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "import glob\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "USxGE9V74L2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for postfix_ in [\"True\", \"False\"]:\n",
        "    sorted_files = sorted(\n",
        "        glob.glob(\n",
        "            f\"/content/COMP8730_research_project/results_scratch_{postfix_}/*/checkpoint*\"\n",
        "        ),\n",
        "        key=lambda x: int(x.split(\"-\")[1]),\n",
        "    )\n",
        "    print(sorted_files[-1])\n",
        "\n",
        "    if postfix_==\"False\":\n",
        "        model_hub_name = \"sanbert-from-indicbert\"\n",
        "    else:\n",
        "        model_hub_name = \"sanbert-from-scratch\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(sorted_files[-1])\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\n",
        "            sorted_files[-1]\n",
        "    )\n",
        "\n",
        "    model.push_to_hub(model_hub_name, use_temp_dir=True)\n",
        "    tokenizer.push_to_hub(model_hub_name, use_temp_dir=True)"
      ],
      "metadata": {
        "id": "GzlnCdvQ2zXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lvv_WlzK3Wqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3jRtJpNo3JjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLIPJcZQ2prb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jLLqnDJW2ppj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "COMP8730_proposed_solution_step2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}